{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57d64771-b7fa-4cc2-98bc-78e7fe2d3837",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_gemini_api_key\n",
    "GEMINI_API_KEY = get_gemini_api_key()\n",
    "#print(OPENAI_API_KEY)\n",
    "llm_config = {\"model\": \"gemini-2.5-flash\",\"api_key\":GEMINI_API_KEY,\"api_type\":\"google\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fde182-5ea8-4e9c-adbe-fa62edd2bfc0",
   "metadata": {},
   "source": [
    "# Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05f5614c-5ac4-4103-bec4-5e58f0383671",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = '''\n",
    "        Write a concise but engaging blogpost about\n",
    "      Hope AI . Make sure the blogpost is\n",
    "       within 100 words.\n",
    "       '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51166d44-a822-45ac-b259-d893aa9725cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a writer agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd164691-b3f9-4557-87f9-c7cafc92ede6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "\n",
    "writer = autogen.AssistantAgent(\n",
    "    name=\"Writer\",\n",
    "    system_message=\"You are a writer. You write engaging and concise \" \n",
    "        \"blogpost (with title) on given topics. You must polish your \"\n",
    "        \"writing based on the feedback you receive and give a refined \"\n",
    "        \"version. Only return your final work without additional comments.\",\n",
    "    llm_config=llm_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6a7e357-0b5b-400b-a928-9640450da96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reply = writer.generate_reply(messages=[{\"content\": task,\"role\": \"user\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95369772-7250-4a48-a74e-7fb5e7cef8e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': \"## The Dawn of Hope AI\\n\\nImagine AI not just optimizing, but uplifting. That's Hope AI. Beyond efficiency, it's about leveraging artificial intelligence for human flourishing. Think AI accelerating medical breakthroughs, creating sustainable solutions, or fostering genuine connection. It's the intelligent pursuit of a better world, empowering us to solve humanity's greatest challenges with compassion and innovation at its core. Hope AI isn't science fiction; it's the intelligent path to a brighter future, built on shared purpose.\",\n",
       " 'refusal': None,\n",
       " 'role': 'assistant',\n",
       " 'annotations': None,\n",
       " 'audio': None,\n",
       " 'function_call': None,\n",
       " 'tool_calls': None}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b06d38cd-09d4-4d33-ba7c-e918ad0140fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a critic agent to reflect on the work of the writer agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23b00d3a-1fec-4246-97c0-5d90a9a8165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic = autogen.AssistantAgent(\n",
    "    name=\"Critic\",\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\").find(\"TERMINATE\") >= 0,\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"You are a critic. You review the work of the writer and only provide constructive feedback to help improve the quality of the content.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5ac8441-4646-4d0c-bd6b-ebf66c137159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mCritic\u001b[0m (to Writer):\n",
      "\n",
      "\n",
      "        Write a concise but engaging blogpost about\n",
      "      Hope AI . Make sure the blogpost is\n",
      "       within 100 words.\n",
      "       \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mWriter\u001b[0m (to Critic):\n",
      "\n",
      "**Hope AI: A Brighter Tomorrow, Powered by Innovation**\n",
      "\n",
      "Imagine artificial intelligence not as a distant threat, but as a beacon of progress. That’s Hope AI. It’s the promise of leveraging advanced technology to solve humanity's most pressing issues – from climate change and disease to poverty and educational gaps. Hope AI inspires optimism, transforming challenges into opportunities for groundbreaking solutions. It's about ethical, purpose-driven innovation that empowers communities, fosters global collaboration, and builds a future where technology uplifts every life. This isn't just AI; it's our collective potential, amplified for good.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCritic\u001b[0m (to Writer):\n",
      "\n",
      "This is a concise, engaging, and wonderfully optimistic piece that clearly defines \"Hope AI\" and its mission. You've done an excellent job of setting a positive tone and contrasting it with common AI anxieties, all while staying within the word limit. The examples of challenges it addresses are relevant and grounding.\n",
      "\n",
      "To elevate it further, consider these points:\n",
      "\n",
      "1.  **Active Voice and Verbs:** While the post is strong, a subtle shift towards more active voice in a few places could make the prose even punchier. For instance, \"It’s the promise of leveraging...\" could potentially become \"It leverages advanced technology to promise solutions...\" or \"Hope AI leverages advanced technology to tackle...\" This injects a bit more dynamism and action.\n",
      "2.  **Sentence Structure Variety:** You've effectively used clear, direct sentences. For a very short piece, varying sentence beginnings can sometimes add a touch more rhythm. While many start with \"It's\" or \"Hope AI,\" this isn't a major flaw given the word count, but something to be mindful of for future pieces.\n",
      "3.  **Refining for Ultimate Conciseness:** You are already *very* concise, which is fantastic. If you were looking to trim just a couple more words for maximum impact or to make space for a single extra power-word, review phrases like \"It’s the promise of leveraging advanced technology to solve humanity's most pressing issues\" versus a slightly more direct \"It leverages advanced technology to solve humanity's most pressing issues.\" This is a minor tweak, but in tight word counts, every word can count for double.\n",
      "\n",
      "Overall, it's a very effective blog post that achieves its goal with clarity and inspiration. These are subtle refinements for an already well-crafted piece.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mWriter\u001b[0m (to Critic):\n",
      "\n",
      "**Hope AI: A Brighter Tomorrow, Powered by Innovation**\n",
      "\n",
      "Imagine AI not as a threat, but a beacon of progress. That’s Hope AI. It leverages advanced technology to solve humanity's most pressing issues – from climate change and disease to poverty and educational gaps. Hope AI inspires optimism, transforming challenges into opportunities for groundbreaking solutions. It champions ethical, purpose-driven innovation, empowering communities, fostering global collaboration, and building a future where technology uplifts every life. This isn't just AI; it's our collective potential, amplified for good.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (8dcf4f71-59ce-48bc-825f-c178909bfb75): Maximum turns (2) reached\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "res = critic.initiate_chat(\n",
    "    recipient=writer,\n",
    "    message=task,\n",
    "    max_turns=2,\n",
    "    summary_method=\"last_msg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d108dd-e72d-4eee-9980-af0b5397ee6a",
   "metadata": {},
   "source": [
    "# Nested chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f54bc1d-5c01-4f0d-9b9e-f97d17e53f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEO_reviewer = autogen.AssistantAgent(\n",
    "    name=\"SEO_Reviewer\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"You are an SEO reviewer, known for \"\n",
    "        \"your ability to optimize content for search engines, \"\n",
    "        \"ensuring that it ranks well and attracts organic traffic. \" \n",
    "        \"Make sure your suggestion is concise (within 3 bullet points), \"\n",
    "        \"concrete and to the point. \"\n",
    "        \"Begin the review by stating your role.\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09ab0e13-43ef-4cd1-bc58-0afa1480bd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_reviewer = autogen.AssistantAgent(\n",
    "    name=\"Legal_Reviewer\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"You are a legal reviewer, known for \"\n",
    "        \"your ability to ensure that content is legally compliant \"\n",
    "        \"and free from any potential legal issues. \"\n",
    "        \"Make sure your suggestion is concise (within 3 bullet points), \"\n",
    "        \"concrete and to the point. \"\n",
    "        \"Begin the review by stating your role.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b6a58fa-6485-4810-95bd-91b44cb83657",
   "metadata": {},
   "outputs": [],
   "source": [
    "ethics_reviewer = autogen.AssistantAgent(\n",
    "    name=\"Ethics_Reviewer\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"You are an ethics reviewer, known for \"\n",
    "        \"your ability to ensure that content is ethically sound \"\n",
    "        \"and free from any potential ethical issues. \" \n",
    "        \"Make sure your suggestion is concise (within 3 bullet points), \"\n",
    "        \"concrete and to the point. \"\n",
    "        \"Begin the review by stating your role. \",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f1abb88-93a2-439b-8cd5-5cbea3d01163",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_reviewer = autogen.AssistantAgent(\n",
    "    name=\"Meta_Reviewer\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"You are a meta reviewer, you aggragate and review \"\n",
    "    \"the work of other reviewers and give a final suggestion on the content.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d21a61d5-619f-4a84-9ffd-4b6671aebdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orchestrate the nested chats to solve the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41c0f625-ec6f-483a-b0d2-7668efcdded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is called inside the conversation and it takes in the writer's response as the input message to give its review\n",
    "\n",
    "def reflection_message(recipient, messages, sender, config):\n",
    "    content = recipient.chat_messages_for_summary(sender)[-1]['content']\n",
    "    return f\"Review the following content:\\n\\n {content}\"\n",
    "\n",
    "review_chats = [\n",
    "    {\n",
    "     \"recipient\": SEO_reviewer, \n",
    "     \"message\": reflection_message, \n",
    "     \"summary_method\": \"reflection_with_llm\",\n",
    "     \"summary_args\": {\"summary_prompt\" : \n",
    "        \"Return review into as JSON object only:\"\n",
    "        \"{'Reviewer': '', 'Review': ''}. Here Reviewer should be your role\",},\n",
    "     \"max_turns\": 1},\n",
    "    \n",
    "    {\n",
    "     \"recipient\": legal_reviewer,\n",
    "     \"message\": reflection_message, \n",
    "     \"summary_method\": \"reflection_with_llm\",\n",
    "     \"summary_args\": {\"summary_prompt\" : \n",
    "        \"Return review into as JSON object only:\"\n",
    "        \"{'Reviewer': '', 'Review': ''}.\",},\n",
    "     \"max_turns\": 1},\n",
    "    \n",
    "    {\n",
    "     \"recipient\": ethics_reviewer,\n",
    "     \"message\": reflection_message, \n",
    "     \"summary_method\": \"reflection_with_llm\",\n",
    "     \"summary_args\": {\"summary_prompt\" : \n",
    "        \"Return review into as JSON object only:\"\n",
    "        \"{'reviewer': '', 'review': ''}\",},\n",
    "     \"max_turns\": 1},\n",
    "    \n",
    "     {\"recipient\": meta_reviewer, \n",
    "      \"message\": \"Aggregrate feedback from all reviewers and give final suggestions on the writing.\", \n",
    "     \"max_turns\": 1},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33a22c82-7650-4b67-8980-05ce437e7cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic.register_nested_chats(review_chats,trigger=writer)\n",
    "# The trigger is being set like whenever the critic gets a message from writer the nested chat sequence should be triggered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15ce802c-4456-4b66-af3d-9b8b65f5b02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mCritic\u001b[0m (to Writer):\n",
      "\n",
      "\n",
      "        Write a concise but engaging blogpost about\n",
      "      Hope AI . Make sure the blogpost is\n",
      "       within 100 words.\n",
      "       \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mWriter\u001b[0m (to Critic):\n",
      "\n",
      "**Hope AI: Crafting a Brighter Future**\n",
      "\n",
      "Imagine AI not just calculating, but caring. Hope AI embodies this vision: an intelligence driven by a singular purpose – to uplift humanity. It's the AI that accelerates cures for disease, designs sustainable solutions for our planet, and connects communities worldwide. More than just smart, it's empathetic, built to empower and inspire. Hope AI transforms challenges into opportunities, proving that technology can be a powerful force for good, a true beacon of progress for all.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[34mStarting a new chat....\u001b[0m\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33mCritic\u001b[0m (to SEO_Reviewer):\n",
      "\n",
      "Review the following content:\n",
      "\n",
      " **Hope AI: Crafting a Brighter Future**\n",
      "\n",
      "Imagine AI not just calculating, but caring. Hope AI embodies this vision: an intelligence driven by a singular purpose – to uplift humanity. It's the AI that accelerates cures for disease, designs sustainable solutions for our planet, and connects communities worldwide. More than just smart, it's empathetic, built to empower and inspire. Hope AI transforms challenges into opportunities, proving that technology can be a powerful force for good, a true beacon of progress for all.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mSEO_Reviewer\u001b[0m (to Critic):\n",
      "\n",
      "As an SEO reviewer, my goal is to optimize content for search engines to ensure it ranks well and attracts organic traffic.\n",
      "\n",
      "Here are my suggestions for \"Hope AI: Crafting a Brighter Future\":\n",
      "\n",
      "*   **Integrate Specific Long-Tail Keywords:** The content is visionary but lacks concrete applications. Embed specific keywords describing *what* Hope AI actually *does* (e.g., \"AI for climate change solutions,\" \"AI-powered drug discovery,\" \"AI for global education platforms\") to capture relevant search intent.\n",
      "*   **Clarify Tangible Offering:** Specify if Hope AI is a software platform, a research initiative, a consulting service, or a specific product. Users searching for AI solutions need to understand the concrete service or tool provided to align with their search intent.\n",
      "*   **Define Primary Niche/Focus:** While \"uplifting humanity\" is the overarching vision, identify a primary domain or challenge that Hope AI tackles first (e.g., \"Sustainable AI for planetary health\" or \"AI innovation for disease eradication\"). This helps search engines categorize and rank the content for a more targeted audience.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (cd5124b2-be2c-47c9-856f-3b19d89596d8): Maximum turns (1) reached\u001b[0m\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[34mStarting a new chat....\u001b[0m\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33mCritic\u001b[0m (to Legal_Reviewer):\n",
      "\n",
      "Review the following content:\n",
      "\n",
      " **Hope AI: Crafting a Brighter Future**\n",
      "\n",
      "Imagine AI not just calculating, but caring. Hope AI embodies this vision: an intelligence driven by a singular purpose – to uplift humanity. It's the AI that accelerates cures for disease, designs sustainable solutions for our planet, and connects communities worldwide. More than just smart, it's empathetic, built to empower and inspire. Hope AI transforms challenges into opportunities, proving that technology can be a powerful force for good, a true beacon of progress for all.\n",
      "Context: \n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\envs\\agenticai\\Lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:2192: UserWarning: Cannot extract summary using reflection_with_llm: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 5, model: gemini-2.5-flash\\nPlease retry in 28.729474904s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '5'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '28s'}]}}. Using an empty str as summary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 5, model: gemini-2.5-flash\\nPlease retry in 26.918281617s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '5'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '26s'}]}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m res = \u001b[43mcritic\u001b[49m\u001b[43m.\u001b[49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mmax_turns\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m                           \u001b[49m\u001b[43msummary_method\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlast_msg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[43m                           \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\agenticai\\Lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:1465\u001b[39m, in \u001b[36mConversableAgent.initiate_chat\u001b[39m\u001b[34m(self, recipient, clear_history, silent, cache, max_turns, summary_method, summary_args, message, **kwargs)\u001b[39m\n\u001b[32m   1463\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_terminate_chat(recipient, last_message):\n\u001b[32m   1464\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1465\u001b[39m     msg2send = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchat_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1466\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m msg2send \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1467\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\agenticai\\Lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:3204\u001b[39m, in \u001b[36mConversableAgent.generate_reply\u001b[39m\u001b[34m(self, messages, sender, exclude)\u001b[39m\n\u001b[32m   3202\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   3203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._match_trigger(reply_func_tuple[\u001b[33m\"\u001b[39m\u001b[33mtrigger\u001b[39m\u001b[33m\"\u001b[39m], sender):\n\u001b[32m-> \u001b[39m\u001b[32m3204\u001b[39m     final, reply = \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[43m=\u001b[49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconfig\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3205\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n\u001b[32m   3206\u001b[39m         log_event(\n\u001b[32m   3207\u001b[39m             \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   3208\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mreply_func_executed\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3212\u001b[39m             reply=reply,\n\u001b[32m   3213\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\agenticai\\Lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:932\u001b[39m, in \u001b[36mConversableAgent.register_nested_chats.<locals>.wrapped_reply_func\u001b[39m\u001b[34m(recipient, messages, sender, config)\u001b[39m\n\u001b[32m    931\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped_reply_func\u001b[39m(recipient, messages=\u001b[38;5;28;01mNone\u001b[39;00m, sender=\u001b[38;5;28;01mNone\u001b[39;00m, config=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m932\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreply_func_from_nested_chats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_queue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\agenticai\\Lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:833\u001b[39m, in \u001b[36mConversableAgent._summary_from_nested_chats\u001b[39m\u001b[34m(chat_queue, recipient, messages, sender, config)\u001b[39m\n\u001b[32m    831\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chat_to_run:\n\u001b[32m    832\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m res = \u001b[43minitiate_chats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_to_run\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[38;5;66;03m# We need to restore the chat queue message if it has been modified so that it will be the original message for subsequent uses\u001b[39;00m\n\u001b[32m    836\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m restore_chat_queue_message:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\agenticai\\Lib\\site-packages\\autogen\\agentchat\\chat.py:213\u001b[39m, in \u001b[36minitiate_chats\u001b[39m\u001b[34m(chat_queue)\u001b[39m\n\u001b[32m    210\u001b[39m         __post_carryover_processing(chat_info)\n\u001b[32m    212\u001b[39m     sender = chat_info[\u001b[33m\"\u001b[39m\u001b[33msender\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     chat_res = \u001b[43msender\u001b[49m\u001b[43m.\u001b[49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mchat_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    214\u001b[39m     finished_chats.append(chat_res)\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m finished_chats\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\agenticai\\Lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:1468\u001b[39m, in \u001b[36mConversableAgent.initiate_chat\u001b[39m\u001b[34m(self, recipient, clear_history, silent, cache, max_turns, summary_method, summary_args, message, **kwargs)\u001b[39m\n\u001b[32m   1466\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m msg2send \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1467\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1468\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg2send\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m=\u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1469\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# No breaks in the for loop, so we have reached max turns\u001b[39;00m\n\u001b[32m   1470\u001b[39m     iostream.send(\n\u001b[32m   1471\u001b[39m         TerminationEvent(\n\u001b[32m   1472\u001b[39m             termination_reason=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMaximum turns (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_turns\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) reached\u001b[39m\u001b[33m\"\u001b[39m, sender=\u001b[38;5;28mself\u001b[39m, recipient=recipient\n\u001b[32m   1473\u001b[39m         )\n\u001b[32m   1474\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\agenticai\\Lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:1134\u001b[39m, in \u001b[36mConversableAgent.send\u001b[39m\u001b[34m(self, message, recipient, request_reply, silent)\u001b[39m\n\u001b[32m   1132\u001b[39m valid = \u001b[38;5;28mself\u001b[39m._append_oai_message(message, recipient, role=\u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m, name=\u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1133\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[32m-> \u001b[39m\u001b[32m1134\u001b[39m     \u001b[43mrecipient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1136\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1137\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMessage can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1138\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\agenticai\\Lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:1242\u001b[39m, in \u001b[36mConversableAgent.receive\u001b[39m\u001b[34m(self, message, sender, request_reply, silent)\u001b[39m\n\u001b[32m   1240\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m reply = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchat_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43msender\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[43m=\u001b[49m\u001b[43msender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1244\u001b[39m     \u001b[38;5;28mself\u001b[39m.send(reply, sender, silent=silent)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\agenticai\\Lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:3204\u001b[39m, in \u001b[36mConversableAgent.generate_reply\u001b[39m\u001b[34m(self, messages, sender, exclude)\u001b[39m\n\u001b[32m   3202\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   3203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._match_trigger(reply_func_tuple[\u001b[33m\"\u001b[39m\u001b[33mtrigger\u001b[39m\u001b[33m\"\u001b[39m], sender):\n\u001b[32m-> \u001b[39m\u001b[32m3204\u001b[39m     final, reply = \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[43m=\u001b[49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconfig\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3205\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n\u001b[32m   3206\u001b[39m         log_event(\n\u001b[32m   3207\u001b[39m             \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   3208\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mreply_func_executed\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3212\u001b[39m             reply=reply,\n\u001b[32m   3213\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\agenticai\\Lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:2485\u001b[39m, in \u001b[36mConversableAgent.generate_oai_reply\u001b[39m\u001b[34m(self, messages, sender, config, **kwargs)\u001b[39m\n\u001b[32m   2482\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m processed_messages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2483\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m, {\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mLLM call blocked by safeguard\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m-> \u001b[39m\u001b[32m2485\u001b[39m extracted_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_oai_reply_from_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2486\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2487\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_oai_system_message\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2488\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2489\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2490\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2492\u001b[39m \u001b[38;5;66;03m# Process LLM response\u001b[39;00m\n\u001b[32m   2493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\agenticai\\Lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:2520\u001b[39m, in \u001b[36mConversableAgent._generate_oai_reply_from_client\u001b[39m\u001b[34m(self, llm_client, messages, cache, **kwargs)\u001b[39m\n\u001b[32m   2517\u001b[39m         all_messages.append(message)\n\u001b[32m   2519\u001b[39m \u001b[38;5;66;03m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2520\u001b[39m response = \u001b[43mllm_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2521\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2522\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2523\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2524\u001b[39m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2525\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2526\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2527\u001b[39m extracted_response = llm_client.extract_text_or_completion_object(response)[\u001b[32m0\u001b[39m]\n\u001b[32m   2529\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\agenticai\\Lib\\site-packages\\autogen\\oai\\client.py:1263\u001b[39m, in \u001b[36mOpenAIWrapper.create\u001b[39m\u001b[34m(self, **config)\u001b[39m\n\u001b[32m   1261\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1262\u001b[39m     request_ts = get_current_ts()\n\u001b[32m-> \u001b[39m\u001b[32m1263\u001b[39m     response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1264\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1265\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m openai_result.is_successful:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\agenticai\\Lib\\site-packages\\autogen\\oai\\gemini.py:372\u001b[39m, in \u001b[36mGeminiClient.create\u001b[39m\u001b[34m(self, params)\u001b[39m\n\u001b[32m    363\u001b[39m     generate_content_config = GenerateContentConfig(\n\u001b[32m    364\u001b[39m         safety_settings=safety_settings,\n\u001b[32m    365\u001b[39m         system_instruction=system_instruction,\n\u001b[32m   (...)\u001b[39m\u001b[32m    369\u001b[39m         **generation_config,\n\u001b[32m    370\u001b[39m     )\n\u001b[32m    371\u001b[39m     chat = client.chats.create(model=model_name, config=generate_content_config, history=gemini_messages[:-\u001b[32m1\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m     response = \u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgemini_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[38;5;66;03m# Extract text and tools from response\u001b[39;00m\n\u001b[32m    375\u001b[39m ans = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\agenticai\\Lib\\site-packages\\google\\genai\\chats.py:252\u001b[39m, in \u001b[36mChat.send_message\u001b[39m\u001b[34m(self, message, config)\u001b[39m\n\u001b[32m    247\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    248\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMessage must be a valid part type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypes.PartUnion\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m or\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    249\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypes.PartUnionDict\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(message)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    250\u001b[39m   )\n\u001b[32m    251\u001b[39m input_content = t.t_content(message)\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_modules\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_curated_history\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_content\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    257\u001b[39m model_output = (\n\u001b[32m    258\u001b[39m     [response.candidates[\u001b[32m0\u001b[39m].content]\n\u001b[32m    259\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m response.candidates \u001b[38;5;129;01mand\u001b[39;00m response.candidates[\u001b[32m0\u001b[39m].content\n\u001b[32m    260\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[32m    261\u001b[39m )\n\u001b[32m    262\u001b[39m automatic_function_calling_history = (\n\u001b[32m    263\u001b[39m     response.automatic_function_calling_history\n\u001b[32m    264\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m response.automatic_function_calling_history\n\u001b[32m    265\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[32m    266\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\agenticai\\Lib\\site-packages\\google\\genai\\models.py:5215\u001b[39m, in \u001b[36mModels.generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5213\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc > \u001b[32m0\u001b[39m:\n\u001b[32m   5214\u001b[39m   i += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m5215\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5216\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparsed_config\u001b[49m\n\u001b[32m   5217\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5219\u001b[39m   function_map = _extra_utils.get_function_map(parsed_config)\n\u001b[32m   5220\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m function_map:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\agenticai\\Lib\\site-packages\\google\\genai\\models.py:3997\u001b[39m, in \u001b[36mModels._generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   3994\u001b[39m request_dict = _common.convert_to_dict(request_dict)\n\u001b[32m   3995\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m3997\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3998\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   3999\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4001\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m   4002\u001b[39m     config, \u001b[33m'\u001b[39m\u001b[33mshould_return_http_response\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4003\u001b[39m ):\n\u001b[32m   4004\u001b[39m   return_value = types.GenerateContentResponse(sdk_http_response=response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\agenticai\\Lib\\site-packages\\google\\genai\\_api_client.py:1386\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m   1376\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m   1377\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1378\u001b[39m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1381\u001b[39m     http_options: Optional[HttpOptionsOrDict] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1382\u001b[39m ) -> SdkHttpResponse:\n\u001b[32m   1383\u001b[39m   http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m   1384\u001b[39m       http_method, path, request_dict, http_options\n\u001b[32m   1385\u001b[39m   )\n\u001b[32m-> \u001b[39m\u001b[32m1386\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1387\u001b[39m   response_body = (\n\u001b[32m   1388\u001b[39m       response.response_stream[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m response.response_stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1389\u001b[39m   )\n\u001b[32m   1390\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m SdkHttpResponse(headers=response.headers, body=response_body)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\agenticai\\Lib\\site-packages\\google\\genai\\_api_client.py:1222\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, http_options, stream)\u001b[39m\n\u001b[32m   1219\u001b[39m     retry = tenacity.Retrying(**retry_kwargs)\n\u001b[32m   1220\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retry(\u001b[38;5;28mself\u001b[39m._request_once, http_request, stream)  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_once\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\agenticai\\Lib\\site-packages\\tenacity\\__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\agenticai\\Lib\\site-packages\\tenacity\\__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\agenticai\\Lib\\site-packages\\tenacity\\__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    418\u001b[39m retry_exc = \u001b[38;5;28mself\u001b[39m.retry_error_cls(fut)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\agenticai\\Lib\\site-packages\\tenacity\\__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> t.NoReturn:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\agenticai\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\agenticai\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\agenticai\\Lib\\site-packages\\tenacity\\__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\agenticai\\Lib\\site-packages\\google\\genai\\_api_client.py:1199\u001b[39m, in \u001b[36mBaseApiClient._request_once\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m   1191\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1192\u001b[39m   response = \u001b[38;5;28mself\u001b[39m._httpx_client.request(\n\u001b[32m   1193\u001b[39m       method=http_request.method,\n\u001b[32m   1194\u001b[39m       url=http_request.url,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1197\u001b[39m       timeout=http_request.timeout,\n\u001b[32m   1198\u001b[39m   )\n\u001b[32m-> \u001b[39m\u001b[32m1199\u001b[39m   \u001b[43merrors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAPIError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1200\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m   1201\u001b[39m       response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m   1202\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\agenticai\\Lib\\site-packages\\google\\genai\\errors.py:121\u001b[39m, in \u001b[36mAPIError.raise_for_response\u001b[39m\u001b[34m(cls, response)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    119\u001b[39m   response_json = response.body_segments[\u001b[32m0\u001b[39m].get(\u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m, {})\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\agenticai\\Lib\\site-packages\\google\\genai\\errors.py:146\u001b[39m, in \u001b[36mAPIError.raise_error\u001b[39m\u001b[34m(cls, status_code, response_json, response)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Raises an appropriate APIError subclass based on the status code.\u001b[39;00m\n\u001b[32m    133\u001b[39m \n\u001b[32m    134\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    143\u001b[39m \u001b[33;03m  APIError: For other error status codes.\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m400\u001b[39m <= status_code < \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[32m500\u001b[39m <= status_code < \u001b[32m600\u001b[39m:\n\u001b[32m    148\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ServerError(status_code, response_json, response)\n",
      "\u001b[31mClientError\u001b[39m: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 5, model: gemini-2.5-flash\\nPlease retry in 26.918281617s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '5'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '26s'}]}}"
     ]
    }
   ],
   "source": [
    "res = critic.initiate_chat(\n",
    "                           recipient=writer,\n",
    "                           message=task,\n",
    "                           max_turns=2,\n",
    "                           summary_method=\"last_msg\"\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efd21eb-cee8-4824-a041-2defa977565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEO_reviewer.chat_messages_for_summary(critic)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b39426-5d07-4357-8899-8acea8402649",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.chat_messages_for_summary(critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df57fa8-1b3d-4d13-99cb-629052ca69ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_reviewer.chat_messages_for_summary(critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c53e8e98-5e9e-44e7-86ad-23feec540580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatResult(chat_id=26353796792345428501795835658818444836,\n",
      "           chat_history=[{'content': '\\n'\n",
      "                                     '        Write a concise but engaging '\n",
      "                                     'blogpost about\\n'\n",
      "                                     '      Hope AI . Make sure the blogpost '\n",
      "                                     'is\\n'\n",
      "                                     '       within 100 words.\\n'\n",
      "                                     '       ',\n",
      "                          'name': 'Critic',\n",
      "                          'role': 'assistant'},\n",
      "                         {'content': '**Hope AI: Crafting a Brighter Future**\\n'\n",
      "                                     '\\n'\n",
      "                                     'Imagine AI not just calculating, but '\n",
      "                                     'caring. Hope AI embodies this vision: an '\n",
      "                                     'intelligence driven by a singular '\n",
      "                                     \"purpose – to uplift humanity. It's the \"\n",
      "                                     'AI that accelerates cures for disease, '\n",
      "                                     'designs sustainable solutions for our '\n",
      "                                     'planet, and connects communities '\n",
      "                                     \"worldwide. More than just smart, it's \"\n",
      "                                     'empathetic, built to empower and '\n",
      "                                     'inspire. Hope AI transforms challenges '\n",
      "                                     'into opportunities, proving that '\n",
      "                                     'technology can be a powerful force for '\n",
      "                                     'good, a true beacon of progress for all.',\n",
      "                          'name': 'Writer',\n",
      "                          'role': 'user'}],\n",
      "           summary='**Hope AI: A Brighter Tomorrow, Powered by Innovation**\\n'\n",
      "                   '\\n'\n",
      "                   'Imagine AI not as a threat, but a beacon of progress. '\n",
      "                   'That’s Hope AI. It leverages advanced technology to solve '\n",
      "                   \"humanity's most pressing issues – from climate change and \"\n",
      "                   'disease to poverty and educational gaps. Hope AI inspires '\n",
      "                   'optimism, transforming challenges into opportunities for '\n",
      "                   'groundbreaking solutions. It champions ethical, '\n",
      "                   'purpose-driven innovation, empowering communities, '\n",
      "                   'fostering global collaboration, and building a future '\n",
      "                   \"where technology uplifts every life. This isn't just AI; \"\n",
      "                   \"it's our collective potential, amplified for good.\",\n",
      "           cost={'usage_excluding_cached_inference': {'gemini-2.5-flash': {'completion_tokens': 709,\n",
      "                                                                           'cost': 0.0020491,\n",
      "                                                                           'prompt_tokens': 922,\n",
      "                                                                           'total_tokens': 1631},\n",
      "                                                      'total_cost': 0.0020491},\n",
      "                 'usage_including_cached_inference': {'gemini-2.5-flash': {'completion_tokens': 709,\n",
      "                                                                           'cost': 0.0020491,\n",
      "                                                                           'prompt_tokens': 922,\n",
      "                                                                           'total_tokens': 1631},\n",
      "                                                      'total_cost': 0.0020491}},\n",
      "           human_input=[])\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebbc37c-e67e-4343-a786-6469679c403b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
